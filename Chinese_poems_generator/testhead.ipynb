{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import tensorflow as tf\n",
    "import numpy as np\n",
    "import argparse\n",
    "import os\n",
    "import random\n",
    "import time\n",
    "import collections\n",
    "\n",
    "type = \"poetrySong\"                   # dataset to use, shijing, songci, etc\n",
    "trainPoems = \"./dataset/\" + type + \"/\" + type + \".txt\" # training file location\n",
    "checkpointsPath = \"./checkpoints/\" + type # checkpoints location"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "訓練樣本總數： 252478\n",
      "測試樣本總數： 0\n"
     ]
    }
   ],
   "source": [
    "isEvaluate=False\n",
    "poems = []\n",
    "file = open(trainPoems, \"r\")\n",
    "for line in file:  #every line is a poem\n",
    "    title, author, poem = line.strip().split(\"::\")  #get title and poem\n",
    "    poem = poem.replace(' ','')\n",
    "    if len(poem) < 10 or len(poem) > 512:  #filter poem\n",
    "        continue\n",
    "    if '_' in poem or '《' in poem or '[' in poem or '(' in poem or '（' in poem:\n",
    "        continue\n",
    "    poem = '[' + poem + ']' #add start and end signs\n",
    "    poems.append(poem)\n",
    "    #print(title, author, poem)\n",
    "\n",
    "#counting words\n",
    "wordFreq = collections.Counter()\n",
    "for poem in poems:\n",
    "    wordFreq.update(poem)\n",
    "# print(wordFreq)\n",
    "\n",
    "# erase words which are not common\n",
    "#--------------------bug-------------------------\n",
    "# word num less than original num, which causes nan value in loss function\n",
    "# erase = []\n",
    "# for key in wordFreq:\n",
    "#     if wordFreq[key] < 2:\n",
    "#         erase.append(key)\n",
    "# for key in erase:\n",
    "#     del wordFreq[key]\n",
    "\n",
    "wordFreq[\" \"] = -1\n",
    "wordPairs = sorted(wordFreq.items(), key = lambda x: -x[1])\n",
    "words, freq = zip(*wordPairs)\n",
    "wordNum = len(words)\n",
    "\n",
    "wordToID = dict(zip(words, range(wordNum))) #word to ID\n",
    "poemsVector = [([wordToID[word] for word in poem]) for poem in poems] # poem to vector\n",
    "if isEvaluate: #evaluating need divide dataset into test set and train set\n",
    "    trainVector = poemsVector[:int(len(poemsVector) * trainRatio)]\n",
    "    testVector = poemsVector[int(len(poemsVector) * trainRatio):]\n",
    "else:\n",
    "    trainVector = poemsVector\n",
    "    testVector = []\n",
    "print(\"訓練樣本總數： %d\" % len(trainVector))\n",
    "print(\"測試樣本總數： %d\" % len(testVector))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    " def buildModel(wordNum, gtX, hidden_units = 128, layers = 2):\n",
    "        \"\"\"build rnn\"\"\"\n",
    "        with tf.variable_scope(\"embedding\"): #embedding\n",
    "            embedding = tf.get_variable(\"embedding\", [wordNum, hidden_units], dtype = tf.float32)\n",
    "            inputbatch = tf.nn.embedding_lookup(embedding, gtX)\n",
    "\n",
    "        basicCell = tf.contrib.rnn.BasicLSTMCell(hidden_units, state_is_tuple = True)\n",
    "        stackCell = tf.contrib.rnn.MultiRNNCell([basicCell] * layers)\n",
    "        initState = stackCell.zero_state(np.shape(gtX)[0], tf.float32)\n",
    "        outputs, finalState = tf.nn.dynamic_rnn(stackCell, inputbatch, initial_state = initState)\n",
    "        outputs = tf.reshape(outputs, [-1, hidden_units])\n",
    "\n",
    "        with tf.variable_scope(\"softmax\"):\n",
    "            w = tf.get_variable(\"w\", [hidden_units, wordNum])\n",
    "            b = tf.get_variable(\"b\", [wordNum])\n",
    "            logits = tf.matmul(outputs, w) + b\n",
    "\n",
    "        probs = tf.nn.softmax(logits)\n",
    "        return logits, probs, stackCell, initState, finalState"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "def probsToWord(weights, words):\n",
    "        \"\"\"probs to word\"\"\"\n",
    "        prefixSum = np.cumsum(weights) #prefix sum\n",
    "        ratio = np.random.rand(1)\n",
    "        index = np.searchsorted(prefixSum, ratio * prefixSum[-1]) # large margin has high possibility to be sampled\n",
    "        return words[index[0]]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "def testHead(characters):\n",
    "        \"\"\"write head poem\"\"\"\n",
    "        print(\"genrating...\")\n",
    "        gtX = tf.placeholder(tf.int32, shape=[1, None])  # input\n",
    "        logits, probs, stackCell, initState, finalState = buildModel(wordNum, gtX)\n",
    "        with tf.Session() as sess:\n",
    "            sess.run(tf.global_variables_initializer())\n",
    "            saver = tf.train.Saver()\n",
    "            checkPoint = tf.train.get_checkpoint_state(checkpointsPath)\n",
    "            # if have checkPoint, restore checkPoint\n",
    "            if checkPoint and checkPoint.model_checkpoint_path:\n",
    "                saver.restore(sess, checkPoint.model_checkpoint_path)\n",
    "                print(\"restored %s\" % checkPoint.model_checkpoint_path)\n",
    "            else:\n",
    "                print(\"no checkpoint found!\")\n",
    "                exit(1)\n",
    "            flag = 1\n",
    "            endSign = {-1: \"，\", 1: \"。\"}\n",
    "            poem = ''\n",
    "            state = sess.run(stackCell.zero_state(1, tf.float32))\n",
    "            x = np.array([[wordToID['[']]])\n",
    "            probs1, state = sess.run([probs, finalState], feed_dict={gtX: x, initState: state})\n",
    "            for word in characters:\n",
    "                if wordToID.get(word) == None:\n",
    "                    print(\"胖虎不认识这个字，你真是文化人！\")\n",
    "                    exit(0)\n",
    "                flag = -flag\n",
    "                while word not in [']', '，', '。', ' ', '？', '！']:\n",
    "                    poem += word\n",
    "                    x = np.array([[wordToID[word]]])\n",
    "                    probs2, state = sess.run([probs, finalState], feed_dict={gtX: x, initState: state})\n",
    "                    word = probsToWord(probs2, words)\n",
    "\n",
    "                poem += endSign[flag]\n",
    "                # keep the context, state must be updated\n",
    "                if endSign[flag] == '。':\n",
    "                    probs2, state = sess.run([probs, finalState],\n",
    "                                             feed_dict={gtX: np.array([[wordToID[\"。\"]]]), initState: state})\n",
    "                    poem += '\\n'\n",
    "                else:\n",
    "                    probs2, state = sess.run([probs, finalState],\n",
    "                                             feed_dict={gtX: np.array([[wordToID[\"，\"]]]), initState: state})\n",
    "\n",
    "            print(characters)\n",
    "            print(poem)\n",
    "            return poem"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "genrating...\n",
      "INFO:tensorflow:Restoring parameters from ./checkpoints/poetrySong/poetrySong-158999\n",
      "restored ./checkpoints/poetrySong/poetrySong-158999\n",
      "你爱我火\n",
      "你至今历拂山前，爱我烦公不此脾。\n",
      "我愿芳香故怜客，火盘陈隶瘦桓黄。\n",
      "\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "'你至今历拂山前，爱我烦公不此脾。\\n我愿芳香故怜客，火盘陈隶瘦桓黄。\\n'"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "testHead(\"你爱我火\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
