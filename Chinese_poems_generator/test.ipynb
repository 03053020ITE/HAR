{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import tensorflow as tf\n",
    "import numpy as np\n",
    "import argparse\n",
    "import os\n",
    "import random\n",
    "import time\n",
    "import collections\n",
    "\n",
    "generateNum = 5                   # number of generated poems per time\n",
    "type = \"poetrySong\"                   # dataset to use, shijing, songci, etc\n",
    "trainPoems = \"./dataset/\" + type + \"/\" + type + \".txt\" # training file location\n",
    "checkpointsPath = \"./checkpoints/\" + type # checkpoints location\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "訓練樣本總數： 252478\n",
      "測試樣本總數： 0\n"
     ]
    }
   ],
   "source": [
    "isEvaluate=False\n",
    "poems = []\n",
    "file = open(trainPoems, \"r\")\n",
    "for line in file:  #every line is a poem\n",
    "    title, author, poem = line.strip().split(\"::\")  #get title and poem\n",
    "    poem = poem.replace(' ','')\n",
    "    if len(poem) < 10 or len(poem) > 512:  #filter poem\n",
    "        continue\n",
    "    if '_' in poem or '《' in poem or '[' in poem or '(' in poem or '（' in poem:\n",
    "        continue\n",
    "    poem = '[' + poem + ']' #add start and end signs\n",
    "    poems.append(poem)\n",
    "    #print(title, author, poem)\n",
    "\n",
    "#counting words\n",
    "wordFreq = collections.Counter()\n",
    "for poem in poems:\n",
    "    wordFreq.update(poem)\n",
    "# print(wordFreq)\n",
    "\n",
    "# erase words which are not common\n",
    "#--------------------bug-------------------------\n",
    "# word num less than original num, which causes nan value in loss function\n",
    "# erase = []\n",
    "# for key in wordFreq:\n",
    "#     if wordFreq[key] < 2:\n",
    "#         erase.append(key)\n",
    "# for key in erase:\n",
    "#     del wordFreq[key]\n",
    "\n",
    "wordFreq[\" \"] = -1\n",
    "wordPairs = sorted(wordFreq.items(), key = lambda x: -x[1])\n",
    "words, freq = zip(*wordPairs)\n",
    "wordNum = len(words)\n",
    "\n",
    "wordToID = dict(zip(words, range(wordNum))) #word to ID\n",
    "poemsVector = [([wordToID[word] for word in poem]) for poem in poems] # poem to vector\n",
    "if isEvaluate: #evaluating need divide dataset into test set and train set\n",
    "    trainVector = poemsVector[:int(len(poemsVector) * trainRatio)]\n",
    "    testVector = poemsVector[int(len(poemsVector) * trainRatio):]\n",
    "else:\n",
    "    trainVector = poemsVector\n",
    "    testVector = []\n",
    "print(\"訓練樣本總數： %d\" % len(trainVector))\n",
    "print(\"測試樣本總數： %d\" % len(testVector))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    " def buildModel(wordNum, gtX, hidden_units = 128, layers = 2):\n",
    "        \"\"\"build rnn\"\"\"\n",
    "        with tf.variable_scope(\"embedding\"): #embedding\n",
    "            embedding = tf.get_variable(\"embedding\", [wordNum, hidden_units], dtype = tf.float32)\n",
    "            inputbatch = tf.nn.embedding_lookup(embedding, gtX)\n",
    "\n",
    "        basicCell = tf.contrib.rnn.BasicLSTMCell(hidden_units, state_is_tuple = True)\n",
    "        stackCell = tf.contrib.rnn.MultiRNNCell([basicCell] * layers)\n",
    "        initState = stackCell.zero_state(np.shape(gtX)[0], tf.float32)\n",
    "        outputs, finalState = tf.nn.dynamic_rnn(stackCell, inputbatch, initial_state = initState)\n",
    "        outputs = tf.reshape(outputs, [-1, hidden_units])\n",
    "\n",
    "        with tf.variable_scope(\"softmax\"):\n",
    "            w = tf.get_variable(\"w\", [hidden_units, wordNum])\n",
    "            b = tf.get_variable(\"b\", [wordNum])\n",
    "            logits = tf.matmul(outputs, w) + b\n",
    "\n",
    "        probs = tf.nn.softmax(logits)\n",
    "        return logits, probs, stackCell, initState, finalState"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "def probsToWord(weights, words):\n",
    "        \"\"\"probs to word\"\"\"\n",
    "        prefixSum = np.cumsum(weights) #prefix sum\n",
    "        ratio = np.random.rand(1)\n",
    "        index = np.searchsorted(prefixSum, ratio * prefixSum[-1]) # large margin has high possibility to be sampled\n",
    "        return words[index[0]]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "def test():\n",
    "        \"\"\"write regular poem\"\"\"\n",
    "        print(\"genrating...\")\n",
    "        gtX = tf.placeholder(tf.int32, shape=[1, None])  # input\n",
    "        logits, probs, stackCell, initState, finalState = buildModel(wordNum, gtX)\n",
    "        with tf.Session() as sess:\n",
    "            sess.run(tf.global_variables_initializer())\n",
    "            saver = tf.train.Saver()\n",
    "            checkPoint = tf.train.get_checkpoint_state(checkpointsPath)\n",
    "            # if have checkPoint, restore checkPoint\n",
    "            if checkPoint and checkPoint.model_checkpoint_path:\n",
    "                saver.restore(sess, checkPoint.model_checkpoint_path)\n",
    "                print(\"restored %s\" % checkPoint.model_checkpoint_path)\n",
    "            else:\n",
    "                print(\"no checkpoint found!\")\n",
    "                exit(1)\n",
    "\n",
    "            poems = []\n",
    "            for i in range(generateNum):\n",
    "                state = sess.run(stackCell.zero_state(1, tf.float32))\n",
    "                x = np.array([[wordToID['[']]]) # init start sign\n",
    "                probs1, state = sess.run([probs, finalState], feed_dict={gtX: x, initState: state})\n",
    "                word = probsToWord(probs1, words)\n",
    "                poem = ''\n",
    "                sentenceNum = 0\n",
    "                while word not in [' ', ']']:\n",
    "                    poem += word\n",
    "                    if word in ['。', '？', '！', '，']:\n",
    "                        sentenceNum += 1\n",
    "                        if sentenceNum % 2 == 0:\n",
    "                            poem += '\\n'\n",
    "                    x = np.array([[wordToID[word]]])\n",
    "                    #print(word)\n",
    "                    probs2, state = sess.run([probs, finalState], feed_dict={gtX: x, initState: state})\n",
    "                    word = probsToWord(probs2, words)\n",
    "                print(poem)\n",
    "                poems.append(poem)\n",
    "            return poems"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "genrating...\n",
      "INFO:tensorflow:Restoring parameters from ./checkpoints/poetrySong/poetrySong-158999\n",
      "restored ./checkpoints/poetrySong/poetrySong-158999\n",
      "戚行岁晚湘林叶，那幸兹风继道吾。\n",
      "更看台台寻相看，从今幽绝契寒青。\n",
      "\n",
      "鹓茸如洗素玉心，满体未从人苦余。\n",
      "盥底汨岌初下日，插虹银面萦罗纹。\n",
      "昨朝翠秀千里重，今秋实是十年春。\n",
      "子孙柏草十二日，南望老翁竞老翁。\n",
      "我今结父良不自，却作田家架上第。\n",
      "见人戏尽未知死，满饭尚令马颠倒。\n",
      "\n",
      "书样金钱不朅梅，嶙张孟乐得亲嘉。\n",
      "三梦清笼赖今日，不炊挈酒可遗味。\n",
      "\n",
      "老掖惟春算，桑绩失偶陪。\n",
      "静从归上担，新栏佐来难。\n",
      "直笑千钱陋，恩休日愈经。\n",
      "三年历辞子，一刻着僧书。\n",
      "\n",
      "竹外云堆城彩低，观鱼父老著翁翁。\n",
      "交成想是春应好，笑趁双鱼石鬭嗤。\n",
      "\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "['戚行岁晚湘林叶，那幸兹风继道吾。\\n更看台台寻相看，从今幽绝契寒青。\\n',\n",
       " '鹓茸如洗素玉心，满体未从人苦余。\\n盥底汨岌初下日，插虹银面萦罗纹。\\n昨朝翠秀千里重，今秋实是十年春。\\n子孙柏草十二日，南望老翁竞老翁。\\n我今结父良不自，却作田家架上第。\\n见人戏尽未知死，满饭尚令马颠倒。\\n',\n",
       " '书样金钱不朅梅，嶙张孟乐得亲嘉。\\n三梦清笼赖今日，不炊挈酒可遗味。\\n',\n",
       " '老掖惟春算，桑绩失偶陪。\\n静从归上担，新栏佐来难。\\n直笑千钱陋，恩休日愈经。\\n三年历辞子，一刻着僧书。\\n',\n",
       " '竹外云堆城彩低，观鱼父老著翁翁。\\n交成想是春应好，笑趁双鱼石鬭嗤。\\n']"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "test()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
