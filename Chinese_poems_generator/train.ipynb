{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import tensorflow as tf\n",
    "import numpy as np\n",
    "import argparse\n",
    "import os\n",
    "import random\n",
    "import time\n",
    "import collections\n",
    "\n",
    "batchSize = 64\n",
    "learningRateBase = 0.001\n",
    "learningRateDecayStep = 1000\n",
    "learningRateDecayRate = 0.95\n",
    "\n",
    "epochNum = 10                    # train epoch\n",
    "generateNum = 5                   # number of generated poems per time\n",
    "\n",
    "type = \"poetrySong\"                   # dataset to use, shijing, songci, etc\n",
    "trainPoems = \"./dataset/\" + type + \"/\" + type + \".txt\" # training file location\n",
    "checkpointsPath = \"./checkpoints/\" + type # checkpoints location\n",
    "saveStep = 1000                   # save model every savestep\n",
    "\n",
    "# evaluate\n",
    "trainRatio = 0.8                    # train percentage\n",
    "evaluateCheckpointsPath = \"./checkpoints/evaluate\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "isEvaluate=False\n",
    "poems = []\n",
    "file = open(trainPoems, \"r\")\n",
    "for line in file:  #every line is a poem\n",
    "    title, author, poem = line.strip().split(\"::\")  #get title and poem\n",
    "    poem = poem.replace(' ','')\n",
    "    if len(poem) < 10 or len(poem) > 512:  #filter poem\n",
    "        continue\n",
    "    if '_' in poem or '《' in poem or '[' in poem or '(' in poem or '（' in poem:\n",
    "        continue\n",
    "    poem = '[' + poem + ']' #add start and end signs\n",
    "    poems.append(poem)\n",
    "    #print(title, author, poem)\n",
    "\n",
    "#counting words\n",
    "wordFreq = collections.Counter()\n",
    "for poem in poems:\n",
    "    wordFreq.update(poem)\n",
    "wordFreq[\" \"] = -1\n",
    "# print(wordFreq)\n",
    "\n",
    "# erase words which are not common\n",
    "#--------------------bug-------------------------\n",
    "# word num less than original num, which causes nan value in loss function\n",
    "# erase = []\n",
    "# for key in wordFreq:\n",
    "#     if wordFreq[key] < 2:\n",
    "#         erase.append(key)\n",
    "# for key in erase:\n",
    "#     del wordFreq[key]\n",
    "# print(wordFreq)\n",
    "\n",
    "\n",
    "wordPairs = sorted(wordFreq.items(), key = lambda x: -x[1])\n",
    "# print(wordPairs)\n",
    "\n",
    "words, freq = zip(*wordPairs)\n",
    "# print(words, freq)\n",
    "\n",
    "wordNum = len(words)\n",
    "\n",
    "wordToID = dict(zip(words, range(wordNum))) #word to ID\n",
    "# print(wordToID)\n",
    "\n",
    "poemsVector = [([wordToID[word] for word in poem]) for poem in poems] # poem to vector\n",
    "if isEvaluate: #evaluating need divide dataset into test set and train set\n",
    "    trainVector = poemsVector[:int(len(poemsVector) * trainRatio)]\n",
    "    testVector = poemsVector[int(len(poemsVector) * trainRatio):]\n",
    "else:\n",
    "    trainVector = poemsVector\n",
    "    testVector = []\n",
    "# print(trainVector[0:100])\n",
    "\n",
    "print(\"訓練樣本總數： %d\" % len(trainVector))\n",
    "print(\"測試樣本總數： %d\" % len(testVector))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def generateBatch(isTrain=True):\n",
    "        #padding length to batchMaxLength\n",
    "        if isTrain:\n",
    "            poemsVector = trainVector\n",
    "        else:\n",
    "            poemsVector = testVector\n",
    "\n",
    "        random.shuffle(poemsVector)\n",
    "        \n",
    "        batchNum = (len(poemsVector) - 1) // batchSize\n",
    "        X = []\n",
    "        Y = []\n",
    "        #create batch\n",
    "        for i in range(batchNum):\n",
    "            batch = poemsVector[i * batchSize: (i + 1) * batchSize]\n",
    "            maxLength = max([len(vector) for vector in batch])\n",
    "            temp = np.full((batchSize, maxLength), wordToID[\" \"], np.int32) # padding space\n",
    "            for j in range(batchSize):\n",
    "                temp[j, :len(batch[j])] = batch[j]\n",
    "            X.append(temp)\n",
    "            temp2 = np.copy(temp) #copy!!!!!!\n",
    "            temp2[:, :-1] = temp[:, 1:]\n",
    "            Y.append(temp2)\n",
    "        return X, Y"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    " def buildModel(wordNum, gtX, hidden_units = 128, layers = 2):\n",
    "        \"\"\"build rnn\"\"\"\n",
    "        with tf.variable_scope(\"embedding\"): #embedding\n",
    "            embedding = tf.get_variable(\"embedding\", [wordNum, hidden_units], dtype = tf.float32)\n",
    "            inputbatch = tf.nn.embedding_lookup(embedding, gtX)\n",
    "\n",
    "        basicCell = tf.contrib.rnn.BasicLSTMCell(hidden_units, state_is_tuple = True)\n",
    "        stackCell = tf.contrib.rnn.MultiRNNCell([basicCell] * layers)\n",
    "        initState = stackCell.zero_state(np.shape(gtX)[0], tf.float32)\n",
    "        outputs, finalState = tf.nn.dynamic_rnn(stackCell, inputbatch, initial_state = initState)\n",
    "        outputs = tf.reshape(outputs, [-1, hidden_units])\n",
    "\n",
    "        with tf.variable_scope(\"softmax\"):\n",
    "            w = tf.get_variable(\"w\", [hidden_units, wordNum])\n",
    "            b = tf.get_variable(\"b\", [wordNum])\n",
    "            logits = tf.matmul(outputs, w) + b\n",
    "\n",
    "        probs = tf.nn.softmax(logits)\n",
    "        return logits, probs, stackCell, initState, finalState"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def train(reload=True):\n",
    "        \"\"\"train model\"\"\"\n",
    "        print(\"training...\")\n",
    "        gtX = tf.placeholder(tf.int32, shape=[batchSize, None])  # input\n",
    "        gtY = tf.placeholder(tf.int32, shape=[batchSize, None])  # output\n",
    "\n",
    "        logits, probs, a, b, c = buildModel(wordNum, gtX)\n",
    "\n",
    "        targets = tf.reshape(gtY, [-1])\n",
    "\n",
    "        #loss\n",
    "        loss = tf.contrib.legacy_seq2seq.sequence_loss_by_example([logits], [targets],\n",
    "                                                                  [tf.ones_like(targets, dtype=tf.float32)])\n",
    "        globalStep = tf.Variable(0, trainable=False)\n",
    "        addGlobalStep = globalStep.assign_add(1)\n",
    "\n",
    "        cost = tf.reduce_mean(loss)\n",
    "        trainableVariables = tf.trainable_variables()\n",
    "        grads, a = tf.clip_by_global_norm(tf.gradients(cost, trainableVariables), 5) \n",
    "        # prevent loss divergence caused by gradient explosion\n",
    "        learningRate = tf.train.exponential_decay(learningRateBase, global_step=globalStep,\n",
    "                                                  decay_steps=learningRateDecayStep, decay_rate=learningRateDecayRate)\n",
    "        optimizer = tf.train.AdamOptimizer(learningRate)\n",
    "        trainOP = optimizer.apply_gradients(zip(grads, trainableVariables))\n",
    "\n",
    "\n",
    "        with tf.Session() as sess:\n",
    "            sess.run(tf.global_variables_initializer())\n",
    "            saver = tf.train.Saver()\n",
    "\n",
    "            if not os.path.exists(checkpointsPath):\n",
    "                os.mkdir(checkpointsPath)\n",
    "\n",
    "            if reload:\n",
    "                checkPoint = tf.train.get_checkpoint_state(checkpointsPath)\n",
    "                # if have checkPoint, restore checkPoint\n",
    "                if checkPoint and checkPoint.model_checkpoint_path:\n",
    "                    saver.restore(sess, checkPoint.model_checkpoint_path)\n",
    "                    print(\"restored %s\" % checkPoint.model_checkpoint_path)\n",
    "                else:\n",
    "                    print(\"no checkpoint found!\")\n",
    "\n",
    "            for epoch in range(epochNum):\n",
    "                X, Y = generateBatch()\n",
    "                epochSteps = len(X) # equal to batch\n",
    "                for step, (x, y) in enumerate(zip(X, Y)):\n",
    "                    a, loss, gStep = sess.run([trainOP, cost, addGlobalStep], feed_dict = {gtX:x, gtY:y})\n",
    "                    print(\"epoch: %d, steps: %d/%d, loss: %3f\" % (epoch + 1, step + 1, epochSteps, loss))\n",
    "                    if gStep % saveStep == saveStep - 1: # prevent save at the beginning\n",
    "                        print(\"save model\")\n",
    "                        saver.save(sess, os.path.join(checkpointsPath, type), global_step=gStep)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
